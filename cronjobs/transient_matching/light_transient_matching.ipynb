{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install --user alerce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# light_transient_matching\n",
    "## Matches DESI observations to ALERCE and DECAM ledger objects\n",
    "\n",
    "This code predominately takes in data from the ALERCE and DECAM ledger brokers and identifies DESI observations within 2 arcseconds of those objects, suspected to be transients. It then prepares those matches to be fed into our [CNN code](https://github.com/MatthewPortman/timedomain/blob/master/cronjobs/transient_matching/modified_cnn_classify_data_gradCAM.ipynb) which attempts to identify the class of these transients.\n",
    "\n",
    "The main matching algorithm uses astropy's **match_coordinate_sky** to match 1-to-1 targets with the objects from the two ledgers. Wrapping functions handle data retrieval from both the ledgers as well as from DESI and prepare this data to be fed into **match_coordinate_sky**. Since ALERCE returns a small enough (pandas) dataframe, we do not need to precondition the input much. However, DECAM has many more objects to match so we use a two-stage process: an initial 2 degree match to tile RA's/DEC's and a second closer 1 arcsecond match to individual targets. \n",
    "\n",
    "As the code is a work in progress, please forgive any redundancies. We are attempting to merge all of the above (neatly) into the same two or three matching/handling functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "from astropy import units as u\n",
    "from astropy.time import Time\n",
    "from astropy.coordinates import SkyCoord, match_coordinates_sky, Angle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import sys\n",
    "\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "from desispec.io import read_spectra, write_spectra\n",
    "from desispec.spectra import Spectra\n",
    "\n",
    "# Some handy global variables\n",
    "global db_filename\n",
    "db_filename = '/global/cfs/cdirs/desi/science/td/daily-search/transients_search.db'\n",
    "global exposure_path\n",
    "exposure_path = os.environ[\"DESI_SPECTRO_REDUX\"]\n",
    "global color_band\n",
    "color_band = \"r\"\n",
    "global minDist\n",
    "minDist = {}\n",
    "\n",
    "global today\n",
    "today = Time.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing the file names\n",
    "def all_candidate_filenames(transient_dir: str):\n",
    "    \n",
    "    # This function grabs the names of all input files in the transient directory and does some python string manipulation\n",
    "    # to grab the names of the input files with full path and the filenames themselves.\n",
    "\n",
    "    try:\n",
    "        filenames_read = glob(transient_dir + \"/*.fits\") # Hardcoding is hopefully a temporary measure.\n",
    "    \n",
    "    except:\n",
    "        print(\"Could not grab/find any fits in the transient spectra directory:\")\n",
    "        print(transient_dir)\n",
    "        filenames_read = [] # Just in case\n",
    "        #filenames_out = [] # Just in case\n",
    "        raise SystemExit(\"Exiting.\")\n",
    "        \n",
    "    #else:\n",
    "        #filenames_out = [s.split(\".\")[0] for s in filenames_read]\n",
    "        #filenames_out = [s.split(\"/\")[-1] for s in filenames_read]\n",
    "        #filenames_out = [s.replace(\"in\", \"out\") for s in filenames_out]\n",
    "        \n",
    "    return filenames_read #, filenames_out\n",
    "\n",
    "#path_to_transient = \"/global/cfs/cdirs/desi/science/td/daily-search/desitrip/out\"\n",
    "#print(all_candidate_filenames(path_to_transient)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From ALeRCE_ledgermaker https://github.com/alercebroker/alerce_client\n",
    "# I have had trouble importing this before so I copy, paste it, and modify it here.\n",
    "\n",
    "# I also leave these imports here because why not?\n",
    "import requests\n",
    "from alerce.core import Alerce\n",
    "from alerce.exceptions import APIError\n",
    "\n",
    "alerce_client = Alerce()\n",
    "\n",
    "# Choose cone_radius of diameter of tile so that, whatever coord I choose for ra_in, dec_in, we cover the whole tile\n",
    "def access_alerts(lastmjd_in=[], ra_in = None, dec_in = None, cone_radius = 3600*4.01, classifier='stamp_classifier', class_names=['SN', 'AGN']):\n",
    "    if type(class_names) is not list:\n",
    "        raise TypeError('Argument `class_names` must be a list.')\n",
    "        \n",
    "    dataframes = []\n",
    "    if not lastmjd_in:\n",
    "        date_range = 60\n",
    "        lastmjd_in = [Time.now().mjd - 60, Time.now().mjd]\n",
    "        print('Defaulting to a lastmjd range of', str(date_range), 'days before today.')\n",
    "        \n",
    "    #print(\"lastmjd:\", lastmjd_in)\n",
    "    for class_name in class_names:\n",
    "        data = alerce_client.query_objects(classifier=classifier,\n",
    "                                           class_name=class_name, \n",
    "                                           lastmjd=lastmjd_in,\n",
    "                                           ra = ra_in,\n",
    "                                           dec = dec_in,\n",
    "                                           radius = cone_radius, # in arcseconds\n",
    "                                           page_size = 5000,\n",
    "                                           order_by='oid',\n",
    "                                           order_mode='DESC',                                          \n",
    "                                           format='pandas')\n",
    "        \n",
    "        #if lastmjd is not None:\n",
    "        #    select = data['lastmjd'] >= lastmjd\n",
    "        #    data = data[select]\n",
    "            \n",
    "        dataframes.append(data)\n",
    "    \n",
    "    #print(pd.concat(dataframes).columns)\n",
    "    return pd.concat(dataframes).sort_values(by = 'lastmjd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/desihub/timedomain/blob/master/too_ledgers/decam_TAMU_ledgermaker.ipynb\n",
    "# Function to grab decam data\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import requests\n",
    "def access_decam_data(url, overwrite=False):\n",
    "    \"\"\"Download reduced DECam transient data from Texas A&M.\n",
    "    Cache the data to avoid lengthy and expensive downloads.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        URL for accessing the data.\n",
    "    overwrite : bool\n",
    "        Download new data and overwrite the cached data.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    decam_transients : pandas.DataFrame\n",
    "        Table of transient data.\n",
    "    \"\"\"\n",
    "    folders = url.split('/')\n",
    "    thedate = folders[-1] if len(folders[-1]) > 0 else folders[-2]\n",
    "    outfile = '{}.csv'.format(thedate)\n",
    "    \n",
    "    if os.path.exists(outfile) and not overwrite:\n",
    "        # Access cached data.\n",
    "        decam_transients = pd.read_csv(outfile)\n",
    "    else:\n",
    "        # Download the DECam data index.\n",
    "        # A try/except is needed because the datahub SSL certificate isn't playing well with URL requests.\n",
    "        try:\n",
    "            decam_dets = requests.get(url, auth=('decam','tamudecam')).text\n",
    "        except:\n",
    "            requests.packages.urllib3.disable_warnings(requests.packages.urllib3.exceptions.InsecureRequestWarning)\n",
    "            decam_dets = requests.get(url, verify=False, auth=('decam','tamudecam')).text\n",
    "            \n",
    "        # Convert transient index page into scrapable data using BeautifulSoup.\n",
    "        soup = BeautifulSoup(decam_dets)\n",
    "        \n",
    "        # Loop through transient object summary JSON files indexed in the main transient page.\n",
    "        # Download the JSONs and dump the info into a Pandas table.\n",
    "        decam_transients = None\n",
    "        j = 0\n",
    "\n",
    "        for a in soup.find_all('a', href=True):\n",
    "            if 'object-summary.json' in a:\n",
    "                link = a['href'].replace('./', '')\n",
    "                summary_url  = url + link        \n",
    "                summary_text = requests.get(summary_url, verify=False, auth=('decam','tamudecam')).text\n",
    "                summary_data = json.loads(summary_text)\n",
    "\n",
    "                j += 1\n",
    "                #print('Accessing {:3d}  {}'.format(j, summary_url)) # Modified by Matt\n",
    "\n",
    "                if decam_transients is None:\n",
    "                    decam_transients = pd.DataFrame(summary_data, index=[0])\n",
    "                else:\n",
    "                    decam_transients = pd.concat([decam_transients, pd.DataFrame(summary_data, index=[0])])\n",
    "                    \n",
    "        # Cache the data for future access.\n",
    "        print('Saving output to {}'.format(outfile))\n",
    "        decam_transients.to_csv(outfile, index=False)\n",
    "        \n",
    "    return decam_transients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to read in fits table info, RA, DEC, MJD and targetid if so desired\n",
    "# Uses control parameter tile to determine if opening tile exposure file or not since headers are different\n",
    "import logging\n",
    "\n",
    "def read_fits_info(filepath: str, transient_candidate = True):\n",
    "    \n",
    "    '''\n",
    "    if transient_candidate:\n",
    "        hdu_num = 1\n",
    "    else:\n",
    "        hdu_num = 5\n",
    "    '''\n",
    "    # Disabling INFO logging temporarily to suppress INFO level output/print from read_spectra\n",
    "    logging.disable(logging.INFO)\n",
    "    try:\n",
    "        spec_info = read_spectra(filepath).fibermap\n",
    "    except:\n",
    "        filename = filepath.split(\"/\")[-1]\n",
    "        print(\"Could not open or use:\", filename)\n",
    "        #print(\"In path:\", filepath)\n",
    "        #print(\"Trying the next file...\")\n",
    "        return np.array([]), np.array([]), 0, 0\n",
    "    \n",
    "    headers = ['TARGETID', 'TARGET_RA', 'TARGET_DEC', 'LAST_MJD']\n",
    "    targ_info = {}\n",
    "    for head in headers:\n",
    "        try:\n",
    "            targ_info[head] = spec_info[head].data\n",
    "        except:\n",
    "            if not head == 'LAST_MJD': print(\"Failed to read in\", head, \"data. Continuing...\")\n",
    "            targ_info[head] = False\n",
    "            \n",
    "        # targ_id = spec_info['TARGETID'].data\n",
    "        # targ_ra = spec_info['TARGET_RA'].data # Now it's a numpy array\n",
    "        # targ_dec = spec_info['TARGET_DEC'].data\n",
    "        # targ_mjd = spec_info['LAST_MJD'] #.data\n",
    "\n",
    "    if np.any(targ_info['LAST_MJD']):\n",
    "        targ_mjd = Time(targ_info['LAST_MJD'][0], format = 'mjd')\n",
    "    elif transient_candidate:\n",
    "        targ_mjd = filepath.split(\"/\")[-1].split(\"_\")[-2] #to grab the date\n",
    "        targ_mjd = Time(targ_mjd, format = 'mjd') #.mjd\n",
    "    else:\n",
    "        print(\"Unable to determine observation mjd for\", filename)\n",
    "        print(\"This target will not be considered.\")\n",
    "        return np.array([]), np.array([]), 0, 0\n",
    "\n",
    "        '''\n",
    "        with fits.open(filepath) as hdu1:\n",
    "            data_table = Table(hdu1[hdu_num].data) #columns\n",
    "        \n",
    "            targ_id = data_table['TARGETID']\n",
    "            targ_ra = data_table['TARGET_RA'].data # Now it's a numpy array\n",
    "            targ_dec = data_table['TARGET_DEC'].data\n",
    "            #targ_mjd = data_table['MJD'][0] some have different versions of this so this is a *bad* idea... at least now I know the try except works!\n",
    "            \n",
    "            if tile:\n",
    "                targ_mjd = hdu1[hdu_num].header['MJD-OBS']\n",
    "        '''\n",
    "    \n",
    "    # if tile and not np.all(targ_mjd):\n",
    "    #     print(\"Unable to grab mjd from spectra, taking it from the filename...\")\n",
    "    #     targ_mjd = filepath.split(\"/\")[-1].split(\"_\")[-2] #to grab the date\n",
    "    #     #targ_mjd = targ_mjd[:4]+\"-\"+targ_mjd[4:6]+\"-\"+targ_mjd[6:] # Adding dashes for Time\n",
    "    #     targ_mjd = Time(targ_mjd, format = 'mjd') #.mjd\n",
    "        \n",
    "    # Re-enabling logging for future calls if necessary\n",
    "    logging.disable(logging.NOTSET)    \n",
    "    \n",
    "    return targ_info[\"TARGET_RA\"], targ_info[\"TARGET_DEC\"], targ_mjd, targ_info[\"TARGETID\"] #targ_ra, targ_dec, targ_mjd, targ_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching function\n",
    "\n",
    "More or less the prototype to the later rendition used for DECAM. Will not be around in later versions of this notebook as I will be able to repurpose the DECAM code to do both. Planned obsolescence? \n",
    "\n",
    "It may not be even worth it at this point... ah well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prototype for the later, heftier matching function\n",
    "# Will be deprecated, please reference commentary in inner_matching later for operation notes\n",
    "def matching(path_in: str, max_sep: float, tile = False, date_dict = {}): \n",
    "    \n",
    "    max_sep *= u.arcsec \n",
    "    #max_sep = Angle(max_sep*u.arcsec)\n",
    "    \n",
    "    #if not target_ra_dec_date:\n",
    "    #    target_ras, target_decs, obs_mjds = read_fits_ra_dec(path_in, tile)\n",
    "    #else:\n",
    "    #    target_ras, target_decs, obs_mjds = target_ra_dec_date\n",
    "    \n",
    "    #Look back 60 days from the DESI observations\n",
    "    days_back = 60\n",
    "        \n",
    "    if not date_dict:\n",
    "        print(\"No RA's/DEC's fed in. Quitting.\")\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    all_trans_matches = []\n",
    "    all_alerts_matches = []\n",
    "    targetid_matches = []\n",
    "    \n",
    "    for obs_mjd, ra_dec in date_dict.items():\n",
    "        \n",
    "        # Grab RAs and DECs from input. \n",
    "        target_ras = ra_dec[:, 0]\n",
    "        target_decs = ra_dec[:, 1]\n",
    "        target_ids = np.int64(ra_dec[:, 2])\n",
    "\n",
    "        # Check for NaN's and remove which don't play nice with match_coordinates_sky\n",
    "        nan_ra = np.isnan(target_ras)\n",
    "        nan_dec = np.isnan(target_decs)\n",
    "\n",
    "        if np.any(nan_ra) or np.any(nan_dec):\n",
    "            print(\"NaNs found, removing them from array (not FITS) before match.\")\n",
    "            #print(\"Original length (ra, dec): \", len(target_ras), len(target_decs))\n",
    "            nans = np.logical_not(np.logical_and(nan_ra, nan_dec))\n",
    "            target_ras = target_ras[nans] # Logic masking, probably more efficient\n",
    "            target_decs = target_decs[nans]\n",
    "            #print(\"Reduced length (ra, dec):\", len(target_ras), len(target_decs))\n",
    "            \n",
    "        # Some code used to test -- please ignore ******************\n",
    "        # Feed average to access alerts, perhaps that will speed things up/find better results\n",
    "        #avg_ra = np.average(target_ras)\n",
    "        #avg_dec = np.average(target_decs)\n",
    "#         coo_trans_search = SkyCoord(target_ras*u.deg, target_decs*u.deg)\n",
    "#         #print(coo_trans_search)\n",
    "#         idxs, d2d, _ = match_coordinates_sky(coo_trans_search, coo_trans_search, nthneighbor = 2)\n",
    "#         # for conesearch in alerce\n",
    "#         max_sep = np.max(d2d).arcsec + 2.1 # to expand a bit further than the furthest neighbor\n",
    "#         ra_in = coo_trans_search[0].ra\n",
    "#         dec_in = coo_trans_search[0].dec\n",
    "        # Some code used to test -- please ignore ******************        \n",
    "        \n",
    "        #print([obs_mjd - days_back, obs_mjd])\n",
    "        try:\n",
    "            alerts = access_alerts(lastmjd_in = [obs_mjd - days_back, obs_mjd], \n",
    "                                   ra_in = target_ras[0], \n",
    "                                   dec_in = target_decs[0], #cone_radius = max_sep, \n",
    "                                   class_names = ['SN']\n",
    "                                   ) # Modified Julian Day .mjd\n",
    "        except:\n",
    "            #print(\"No SN matches (\"+str(days_back)+\" day range) for\", obs_mjd)\n",
    "            #break\n",
    "            continue\n",
    "    \n",
    "        # For each fits file, look at one month before the observation from Alerce\n",
    "        # Not sure kdtrees matter\n",
    "        # tree_name = \"kdtree_\" + str(obs_mjd - days_back)\n",
    "\n",
    "        alerts_ra = alerts['meanra'].to_numpy()\n",
    "        #print(\"Length of alerts: \", len(alerts_ra))\n",
    "        alerts_dec = alerts['meandec'].to_numpy()\n",
    "\n",
    "        # Converting to SkyCoord type arrays (really quite handy)\n",
    "        coo_trans_search = SkyCoord(target_ras*u.deg, target_decs*u.deg)\n",
    "        coo_alerts = SkyCoord(alerts_ra*u.deg, alerts_dec*u.deg)\n",
    "        \n",
    "        # Some code used to test -- please ignore ******************\n",
    "        #ra_range = list(zip(*[(i, j) for i,j in zip(alerts_ra,alerts_dec) if (np.min(target_ras) < i and i < np.max(target_ras) and np.min(target_decs) < j and j < np.max(target_decs))]))\n",
    "        #try: \n",
    "        #    ra_range = SkyCoord(ra_range[0]*u.deg, ra_range[1]*u.deg)\n",
    "        #except:\n",
    "        #    continue\n",
    "        #print(ra_range)\n",
    "        #print(coo_trans_search)\n",
    "        #idx_alerts, d2d_trans, d3d_trans = match_coordinates_sky(coo_trans_search, ra_range)\n",
    "        #for i in coo_trans_search:\n",
    "            #print(i.separation(ra_range[3]))\n",
    "        #print(idx_alerts)\n",
    "        #print(np.min(d2d_trans))\n",
    "        #break\n",
    "        # Some code used to test -- please ignore ******************\n",
    "        \n",
    "        idx_alerts, d2d_trans, d3d_trans = match_coordinates_sky(coo_trans_search, coo_alerts) \n",
    "\n",
    "        # Filtering by maximum separation and closest match\n",
    "        sep_constraint = d2d_trans < max_sep\n",
    "        trans_matches = coo_trans_search[sep_constraint]\n",
    "        alerts_matches = coo_alerts[idx_alerts[sep_constraint]]\n",
    "        \n",
    "        targetid_matches = target_ids[sep_constraint]\n",
    "        \n",
    "        #print(d2d_trans < max_sep)\n",
    "        minDist[obs_mjd] = np.min(d2d_trans)\n",
    "\n",
    "        # Adding everything to lists and outputting\n",
    "        if trans_matches.size:\n",
    "            all_trans_matches.append(trans_matches)\n",
    "            all_alerts_matches.append(alerts_matches)\n",
    "            sort_dist = np.sort(d2d_trans)\n",
    "            #print(\"Minimum distance found: \", sort_dist[0])\n",
    "            #print()\n",
    "            #break\n",
    "        #else:\n",
    "            #print(\"No matches found...\\n\")\n",
    "            #break\n",
    "\n",
    "    return all_trans_matches, all_alerts_matches, targetid_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching to ALERCE \n",
    "Runs a 5 arcsecond match of DESI to Alerce objects. Since everything is handled in functions, this part is quite clean.\n",
    "\n",
    "From back when I was going to use *if __name__ == \"__main__\":*... those were the days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transient dir\n",
    "path_to_transient = \"/global/cfs/cdirs/desi/science/td/daily-search/desitrip/out\"\n",
    "# Grab paths\n",
    "paths_to_fits = all_candidate_filenames(path_to_transient)\n",
    "#print(len(paths_to_fits))\n",
    "\n",
    "desi_info_dict = {}\n",
    "target_ras, target_decs, obs_mjd, targ_ids = read_fits_info(paths_to_fits[0], transient_candidate = True)\n",
    "desi_info_dict[obs_mjd] = np.column_stack((target_ras, target_decs, targ_ids))\n",
    "\n",
    "'''\n",
    "To be used when functions are properly combined.\n",
    "initial_check(ledger_df = None, ledger_type = '')\n",
    "closer_check(matches_dict = {}, ledger_df = None, ledger_type = '', exclusion_list = [])\n",
    "'''\n",
    "fail_count = 0\n",
    "# Iterate through every fits file and grab all necessary info and plop it all together\n",
    "for path in paths_to_fits[1:]:\n",
    "    target_ras, target_decs, obs_mjd, targ_ids = read_fits_info(path, transient_candidate = True) \n",
    "\n",
    "    if not obs_mjd: \n",
    "        fail_count += 1\n",
    "        continue\n",
    "\n",
    "    #try:\n",
    "    if obs_mjd in desi_info_dict.keys():\n",
    "        np.append(desi_info_dict[obs_mjd], np.array([target_ras, target_decs, targ_ids]).T, axis = 0)\n",
    "    else:\n",
    "        desi_info_dict[obs_mjd] = np.column_stack((target_ras, target_decs, targ_ids))\n",
    "        #desi_info_dict[obs_mjd].extend((target_ras, target_decs, targ_ids))\n",
    "    #except:\n",
    "    #    continue\n",
    "        #desi_info_dict[obs_mjd] = np.column_stack((target_ras, target_decs, targ_ids))\n",
    "        #desi_info_dict[obs_mjd].append((target_ras, target_decs, targ_ids))\n",
    "#trans_matches, _ = matching(path, 5.0, (all_desi_ras, all_desi_decs, all_obs_mjd))\n",
    "\n",
    "#     if trans_matches.size:\n",
    "#         all_trans_matches.append(trans_matches)\n",
    "#         all_alerts_matches.append(alerts_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1213\n",
      "741\n"
     ]
    }
   ],
   "source": [
    "#print([i.mjd for i in sorted(desi_info_dict.keys())])\n",
    "print(len(paths_to_fits))\n",
    "print(len(desi_info_dict))\n",
    "#print(fail_count)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#print(len(desi_info_dict))\n",
    "temp_dict = {a:b for a,b,c in zip(desi_info_dict.keys(), desi_info_dict.values(), range(len(desi_info_dict))) if c > 650}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# I was going to prepare everything by removing duplicate target ids but it's more trouble than it's worth and match_coordinates_sky can handle it\n",
    "# Takes quite a bit of time... not much more I can do to speed things up though since querying Alerce for every individual date is the hang-up.\n",
    "#print(len(paths_to_fits) - ledesi_info_dictfo_dict))\n",
    "#print(fail_count)\n",
    "\n",
    "#trans_matches, _, target_id_matches = matching(\"\", 2.0, date_dict = temp_dict)\n",
    "trans_matches, _, target_id_matches = matching(\"\", 2.0, date_dict = desi_info_dict)\n",
    "\n",
    "print(trans_matches)\n",
    "print(target_id_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Angle 0.00233529 deg>, <Angle 0.00512101 deg>, <Angle 0.00519003 deg>, <Angle 0.00640351 deg>, <Angle 0.0124173 deg>]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(minDist.values())[:5])\n",
    "#for i in minDist.values():\n",
    "#    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching to DECAM functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overwrite *read_fits_info* with older version to accommodate *read_spectra* error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read useful data from fits file, RA, DEC, target ID, and mjd as a leftover from previous use \n",
    "def read_fits_info(filepath: str, transient_candidate = False):\n",
    "    \n",
    "    if transient_candidate:\n",
    "        hdu_num = 1\n",
    "    else:\n",
    "        hdu_num = 5\n",
    "    \n",
    "    try:\n",
    "        with fits.open(filepath) as hdu1:\n",
    "    \n",
    "            data_table = Table(hdu1[hdu_num].data) #columns\n",
    "        \n",
    "            targ_ID = data_table['TARGETID']\n",
    "            targ_ra = data_table['TARGET_RA'].data # Now it's a numpy array\n",
    "            targ_dec = data_table['TARGET_DEC'].data\n",
    "            \n",
    "            #targ_mjd = data_table['MJD'][0] some have different versions of this so this is a *bad* idea... at least now I know the try except works!\n",
    "            \n",
    "            # if transient_candidate: \n",
    "            #     targ_mjd = hdu1[hdu_num].header['MJD-OBS'] # This is a string\n",
    "            # else:\n",
    "            #     targ_mjd = data_table['MJD'].data\n",
    "            #     targ_mjd = Time(targ_mjd[0], format = 'mjd')\n",
    "            \n",
    "    except:\n",
    "        filename = filepath.split(\"/\")[-1]\n",
    "        print(\"Could not open or use:\", filename)\n",
    "        #print(\"In path:\", filepath)\n",
    "        #print(\"Trying the next file...\")\n",
    "        return np.array([]), np.array([]), np.array([])\n",
    "    \n",
    "    return targ_ra, targ_dec, targ_ID #targ_mjd, targ_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing the frame fits files\n",
    "def glob_frames(exp_d: str):   \n",
    "    \n",
    "    # This function grabs the names of all input files in the transient directory and does some python string manipulation\n",
    "    # to grab the names of the input files with full path and the filenames themselves.\n",
    "\n",
    "    try:\n",
    "        filenames_read = glob(exp_d + \"/cframe-\" + color_band + \"*.fits\") # Only need one of b, r, z\n",
    "        # sframes not flux calibrated\n",
    "        # May want to use tiles... coadd (will need later, but not now)\n",
    "    except:\n",
    "        try:\n",
    "            filenames_read = glob(exp_d + \"/frame-\" + color_band + \"*.fits\") # Only need one of b, r, z\n",
    "        except:\n",
    "            print(\"Could not grab/find any fits in the exposure directory:\")\n",
    "            print(exp_d)\n",
    "            filenames_read = [] # Just in case\n",
    "            #filenames_out = [] # Just in case\n",
    "            raise SystemExit(\"Exitting.\")\n",
    "\n",
    "    #else:\n",
    "        #filenames_out = [s.split(\".\")[0] for s in filenames_read]\n",
    "        #filenames_out = [s.split(\"/\")[-1] for s in filenames_read]\n",
    "        #filenames_out = [s.replace(\"in\", \"out\") for s in filenames_out]\n",
    "        \n",
    "    return filenames_read #, filenames_out\n",
    "\n",
    "#path_to_transient = \"/global/cfs/cdirs/desi/science/td/daily-search/desitrip/out\"\n",
    "#print(all_candidate_filenames(path_to_transient)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match handling routines\n",
    "\n",
    "The two functions below perform data handling/calling for the final match step. \n",
    "\n",
    "The first, **initial_check** grabs all the tile RAs and DECS from the exposures and tiles SQL table, does some filtering, and sends the necessary information to the matching function. Currently designed to handle ALERCE as well but work has to be done to make sure it operates correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_check(ledger_df = None, ledger_type = ''):\n",
    "\n",
    "    query_date_start = \"20210301\"\n",
    "    \n",
    "    #today = Time.now()\n",
    "    smushed_YMD = today.iso.split(\" \")[0].replace(\"-\",\"\")\n",
    "    \n",
    "    query_date_end = smushed_YMD \n",
    "\n",
    "    # Handy queries for debugging/useful info\n",
    "    query2 = \"PRAGMA table_info(exposures)\"\n",
    "    query3 = \"PRAGMA table_info(tiles)\"\n",
    "    # Crossmatch across tiles and exposures to grab obsdate via tileid\n",
    "    query_match = \"SELECT distinct tilera, tiledec, obsdate, obsmjd, expid, exposures.tileid from exposures INNER JOIN tiles ON exposures.tileid = tiles.tileid where obsdate BETWEEN \" + \\\n",
    "        query_date_start + \" AND \" + query_date_end + \";\" \n",
    "    \n",
    "    '''\n",
    "    Some handy code for debugging\n",
    "    #cur.execute(query2)\n",
    "    #row2 = cur.fetchall()\n",
    "    #for i in row2:\n",
    "    #    print(i[:])\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # Querying sql and returning a data type called sqlite3 row, it's kind of like a namedtuple/dictionary\n",
    "    conn = sqlite3.connect(db_filename)\n",
    "\n",
    "    conn.row_factory = sqlite3.Row # https://docs.python.org/3/library/sqlite3.html#sqlite3.Row\n",
    "\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    cur.execute(query_match)\n",
    "    matches_list = cur.fetchall()\n",
    "    cur.close()\n",
    "\n",
    "    # I knew there was a way! THANK YOU!\n",
    "    # https://stackoverflow.com/questions/11276473/append-to-a-dict-of-lists-with-a-dict-comprehension\n",
    "    \n",
    "    # Grabbing everything by obsdate from matches_list\n",
    "    date_dict = {k['obsdate'] : list(filter(lambda x:x['obsdate'] == k['obsdate'], matches_list)) for k in matches_list}\n",
    "\n",
    "    alert_matches_dict = {}\n",
    "\n",
    "    all_trans_matches = []\n",
    "    all_alerts_matches = []\n",
    "    \n",
    "    # Grabbing DECAM ledger if not already fed in\n",
    "    if ledger_type.upper() == 'DECAM_TAMU':\n",
    "        if ledger_df.empty:\n",
    "            ledger_df = access_decam_data('https://datahub.geos.tamu.edu:8000/decam/LCData_Legacy/')\n",
    "\n",
    "    # Iterating through the dates and checking each tile observed on each date\n",
    "    # It is done in this way to cut down on calls to ALERCE since we go day by day\n",
    "    # It's also a convenient way to organize things\n",
    "    for date, row in date_dict.items():\n",
    "        \n",
    "        date_str = str(date)\n",
    "        date_str = date_str[:4]+\"-\"+date_str[4:6]+\"-\"+date_str[6:] # Adding dashes for Time\n",
    "        obs_mjd = Time(date_str).mjd\n",
    "\n",
    "        # This method is *technically* safer than doing a double list comprehension with set albeit slower\n",
    "        # The lists are small enough that speed shouldn't matter here\n",
    "        unique_tileid = {i['tileid']: (i['tilera'], i['tiledec']) for i in row}\n",
    "        exposure_ras, exposure_decs = zip(*unique_tileid.values())\n",
    "        # Grabbing alerce ledger if not done already\n",
    "        if ledger_type.upper() == 'ALERCE':\n",
    "            if ledger_df.empty:\n",
    "                ledger_df = access_alerts(lastmjd = obs_mjd - 28) # Modified Julian Day #.mjd\n",
    "        elif ledger_type.upper() == 'DECAM_TAMU':\n",
    "            pass\n",
    "        else:\n",
    "            print(\"Cannot use alerts broker/ledger provided. Stopping before match.\")\n",
    "            return {}\n",
    "\n",
    "        #Reatin tileid\n",
    "        tileid_arr = np.array(list(unique_tileid.keys())) \n",
    "\n",
    "        # Where the magic/matching happens\n",
    "        trans_matches, alert_matches, trans_ids, alerts_ids, _ = \\\n",
    "            inner_matching(target_ids_in = tileid_arr, target_ras_in = exposure_ras, target_decs_in = exposure_decs, obs_mjd_in = obs_mjd, \n",
    "            path_in = '', max_sep = 1.8, sep_units = 'deg', ledger_df_in = ledger_df, ledger_type_in = ledger_type)\n",
    "        \n",
    "        # Add everything into one giant list for both\n",
    "        if trans_matches.size:\n",
    "            #print(date, \"-\", len(trans_matches), \"matches\")\n",
    "            all_trans_matches.append(trans_matches)\n",
    "            all_alerts_matches.append(alert_matches)\n",
    "        else:\n",
    "            #print(\"No matches on\", date)\n",
    "            continue\n",
    "\n",
    "        # Prepping output\n",
    "        # Populating the dictionary by date (a common theme)\n",
    "        # Each element in the dictionary thus contains the entire sqlite3 row (all info from sql tables with said headers)\n",
    "        alert_matches_dict[date] = []\n",
    "\n",
    "        for tup in trans_matches:\n",
    "            ra = tup.ra.deg\n",
    "            dec = tup.dec.deg\n",
    "            match_rows = [i for i in row if (i['tilera'], i['tiledec']) == (ra, dec)] # Just rebuilding for populating, this shouldn't change/exclude anything\n",
    "            alert_matches_dict[date].extend(match_rows)\n",
    "            \n",
    "    return alert_matches_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## closer_check\n",
    "**closer_check** is also a handling function but operates differently in that now it is checking individual targets. This *must* be run after **initial_check** because it takes as input the dictionary **initial_check** spits out. It then grabs all the targets from the DESI files and pipes that into the matching function but this time with a much more strict matching radius (in this case 2 arcseconds). \n",
    "\n",
    "It then preps the data for output and writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closer_check(matches_dict = {}, ledger_df = None, ledger_type = '', exclusion_list = []):\n",
    "    all_exp_matches = {}\n",
    "        \n",
    "    if not matches_dict:\n",
    "        print(\"No far matches fed in for nearby matching. Returning none.\")\n",
    "        return {}\n",
    "    \n",
    "    # Again just in case the dataframe isn't fed in\n",
    "    if ledger_type.upper() == 'DECAM_TAMU':\n",
    "        \n",
    "        id_head = 'ObjectID'\n",
    "        ra_head = 'RA-OBJECT'\n",
    "        dec_head = 'DEC-OBJECT'\n",
    "        \n",
    "        if ledger_df.empty:\n",
    "            ledger_df = access_decam_data('https://datahub.geos.tamu.edu:8000/decam/LCData_Legacy/')\n",
    "    \n",
    "    count_flag=0\n",
    "    # Iterating through date and all tile information for that date\n",
    "    for date, row in matches_dict.items(): \n",
    "        print(\"\\n\", date)\n",
    "        if date in exclusion_list:\n",
    "            continue\n",
    "\n",
    "        # Declaring some things\n",
    "        all_exp_matches[date] = []\n",
    "        alert_exp_matches = []\n",
    "        file_indices = {}\n",
    "\n",
    "        all_targ_ras = np.array([])\n",
    "        all_targ_decs = np.array([])\n",
    "        all_targ_ids = np.array([])\n",
    "        all_tileids = np.array([])\n",
    "        all_petals = np.array([])\n",
    "\n",
    "        # Iterating through each initial match tile for every date\n",
    "        for i in row:\n",
    "            # Grabbing the paths and iterating through them to grab the RA's/DEC's\n",
    "            exp_paths = '/'.join((exposure_path, \"daily/exposures\", str(i['obsdate']), \"000\"+str(i['expid'])))\n",
    "            #print(exp_paths)\n",
    "            for path in glob_frames(exp_paths):\n",
    "                #print(path)\n",
    "                targ_ras, targ_decs, targ_ids = read_fits_info(path, transient_candidate = False)\n",
    "                \n",
    "                h=fits.open(path)\n",
    "                tileid = h[0].header['TILEID']\n",
    "                tileids = np.full(len(targ_ras),tileid).tolist()\n",
    "                petal = path.split(\"/\")[-1].split(\"-\")[1][-1]\n",
    "                petals = np.full(len(targ_ras),petal).tolist()\n",
    "\n",
    "                # This is to retain the row to debug/check the original FITS file\n",
    "                # And to pull the info by row direct if you feel so inclined\n",
    "                all_len = len(all_targ_ras)\n",
    "                new_len = len(targ_ras)\n",
    "                if all_len:\n",
    "                    all_len -= 1\n",
    "                    file_indices[path] = (all_len, all_len + new_len) # The start and end index, modulo number\n",
    "                else:\n",
    "                    file_indices[path] = (0, new_len) # The start and end index, modulo number\n",
    "\n",
    "                if len(targ_ras) != len(targ_decs):\n",
    "                    print(\"Length of all ras vs. all decs do not match.\")\n",
    "                    print(\"Something went wrong!\")\n",
    "                    print(\"Continuing but not adding those to match...\")\n",
    "                    continue\n",
    "\n",
    "                # All the ras/decs together!\n",
    "                all_targ_ras = np.append(all_targ_ras, targ_ras)\n",
    "                all_targ_decs = np.append(all_targ_decs, targ_decs)\n",
    "                all_targ_ids = np.append(all_targ_ids, targ_ids)\n",
    "                all_tileids = np.append(all_tileids, tileids)\n",
    "                all_petals = np.append(all_petals, petals)\n",
    "\n",
    "        date_mjd = str(date)[:4]+\"-\"+str(date)[4:6] + \"-\" + str(date)[6:] # Adding dashes for Time\n",
    "        date_mjd = Time(date_mjd).mjd\n",
    "        \n",
    "        # Grabbing ALERCE just in case\n",
    "        # Slow\n",
    "        if ledger_type.upper() == 'ALERCE':\n",
    "            \n",
    "            id_head = 'oid'\n",
    "            ra_head = 'meanra'\n",
    "            dec_head = 'meandec'\n",
    "            \n",
    "            if ledger_df.empty:\n",
    "                ledger_df = access_alerts(lastmjd_in = obs_mjd - 45) # Modified Julian Day #.mjd\n",
    "                \n",
    "        # Checking for NaNs, again doesn't play nice with match_coordinates_sky\n",
    "        nan_ra = np.isnan(all_targ_ras)\n",
    "        nan_dec = np.isnan(all_targ_decs)\n",
    "    \n",
    "        if np.any(nan_ra) or np.any(nan_dec):\n",
    "            print(\"NaNs found, removing them from array before match.\")\n",
    "            #print(\"Original length (ra, dec): \", len(target_ras), len(target_decs))\n",
    "            nans = np.logical_not(np.logical_and(nan_ra, nan_dec))\n",
    "            all_targ_ras = all_targ_ras[nans] # Logic masking, probably more efficient\n",
    "            all_targ_decs = all_targ_decs[nans]\n",
    "            all_targ_ids = all_targ_ids[nans]\n",
    "            all_tileids = all_tileids[nans]\n",
    "            all_petals = all_petals[nans]\n",
    "        \n",
    "        # Where the magic matching happens. This time with separation 2 arcseconds.\n",
    "        # Will be cleaned up (eventually)\n",
    "        alert_exp_matches, alerts_matches, targetid_exp_matches, id_alerts_matches, exp_idx = inner_matching(target_ids_in =all_targ_ids, \\\n",
    "                        target_ras_in = all_targ_ras, target_decs_in = all_targ_decs, obs_mjd_in = date_mjd, \n",
    "                        path_in = '', max_sep = 2, sep_units = 'arcsec', ledger_df_in = ledger_df, ledger_type_in = ledger_type)\n",
    "        \n",
    "        date_arr=np.full(alerts_matches.shape[0],date)\n",
    "        #print(date_arr.shape,targetid_exp_matches.shape,alert_exp_matches.shape, id_alerts_matches.shape,alerts_matches.shape )\n",
    "        info_arr_date=np.column_stack((date_arr,all_tileids[exp_idx],all_petals[exp_idx], targetid_exp_matches,alert_exp_matches.ra.deg,alert_exp_matches.dec.deg, \\\n",
    "                                       id_alerts_matches,alerts_matches.ra.deg,alerts_matches.dec.deg ))\n",
    "        all_exp_matches[date].append(info_arr_date)\n",
    "        \n",
    "        if count_flag==0: \n",
    "            all_exp_matches_arr=info_arr_date\n",
    "            count_flag=1\n",
    "        else: \n",
    "            #print(all_exp_matches_arr,info_arr_date)\n",
    "            all_exp_matches_arr=np.concatenate((all_exp_matches_arr,info_arr_date))\n",
    "    \n",
    "    # Does not easily output to a csv since we have multiple results for each date\n",
    "    # so uh... custom file output for me\n",
    "    return all_exp_matches_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inner_matching\n",
    "#### aka the bread & butter\n",
    "**inner_matching** is what ultimately does the final match and calls **match_coordinates_sky** with everything fed in. So really it doesn't do much other than take in all the goodies and make everyone happy.\n",
    "\n",
    "It may still be difficult to co-opt for alerce matching but that may be a project for another time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_matching(target_ids_in = np.array([]), target_ras_in = np.array([]), target_decs_in = np.array([]), obs_mjd_in = '', path_in = '', max_sep = 2, sep_units = 'arcsec', ledger_df_in = None, ledger_type_in = ''): # to be combined with the other matching thing in due time\n",
    "    \n",
    "    # Figuring out the units\n",
    "    if sep_units == 'arcsec':\n",
    "        max_sep *= u.arcsec\n",
    "    elif sep_units == 'arcmin':\n",
    "        max_sep *= u.arcmin\n",
    "    elif sep_units == 'deg':\n",
    "        max_sep *= u.deg\n",
    "    else:\n",
    "        print(\"Separation unit specified is invalid for matching. Defaulting to arcsecond.\")\n",
    "        max_sep *= u.arcsec\n",
    "        \n",
    "    if not np.array(target_ras_in).size:\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    # Checking for NaNs, again doesn't play nice with match_coordinates_sky\n",
    "    nan_ra = np.isnan(target_ras_in)\n",
    "    nan_dec = np.isnan(target_decs_in)\n",
    "    \n",
    "    if np.any(nan_ra) or np.any(nan_dec):\n",
    "        print(\"NaNs found, removing them from array before match.\")\n",
    "        #print(\"Original length (ra, dec): \", len(target_ras), len(target_decs))\n",
    "        nans = np.logical_not(np.logical_and(nan_ra, nan_dec))\n",
    "        target_ras_in = target_ras_in[nans] # Logic masking, probably more efficient\n",
    "        target_decs_in = target_decs_in[nans]\n",
    "        target_ids_in = target_ids_in[nans]\n",
    "        \n",
    "        #print(\"Reduced length (ra, dec):\", len(target_ras), len(target_decs))\n",
    "\n",
    "    # For quick matching if said kdtree actually does anything\n",
    "    # Supposed to speed things up on subsequent runs *shrugs*\n",
    "    tree_name = \"_\".join((\"kdtree\", ledger_type_in, str(obs_mjd_in)))\n",
    "    \n",
    "    # Selecting header string to use with the different alert brokers/ledgers\n",
    "    if ledger_type_in.upper() == 'DECAM_TAMU':\n",
    "        id_head = 'ObjectID'\n",
    "        ra_head = 'RA-OBJECT'\n",
    "        dec_head = 'DEC-OBJECT'\n",
    "    \n",
    "    elif ledger_type_in.upper() == 'ALERCE':\n",
    "        id_head = 'oid' #Check this is how id is called!\n",
    "        ra_head = 'meanra'\n",
    "        dec_head = 'meandec'\n",
    "        \n",
    "    else:\n",
    "        print(\"No ledger type specified. Quitting.\") \n",
    "        # lofty goals\n",
    "        # Will try to figure it out assuming it's a pandas dataframe.\")\n",
    "        #print(\"Returning empty-handed for now until that is complete - Matthew P.\")\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    # Convert df RA/DEC to numpy arrays\n",
    "    alerts_id = ledger_df_in[id_head].to_numpy()\n",
    "    alerts_ra = ledger_df_in[ra_head].to_numpy()\n",
    "    alerts_dec = ledger_df_in[dec_head].to_numpy()\n",
    "\n",
    "    # Convert everything to SkyCoord\n",
    "    coo_trans_search = SkyCoord(target_ras_in*u.deg, target_decs_in*u.deg)\n",
    "    coo_alerts = SkyCoord(alerts_ra*u.deg, alerts_dec*u.deg)\n",
    "\n",
    "    # Do the matching! \n",
    "    idx_alerts, d2d_trans, d3d_trans = match_coordinates_sky(coo_trans_search, coo_alerts, storekdtree = tree_name) # store tree to speed up subsequent results\n",
    "\n",
    "    # Filter out the good stuff\n",
    "    sep_constraint = d2d_trans < max_sep\n",
    "    trans_matches = coo_trans_search[sep_constraint]\n",
    "    trans_matches_ids = target_ids_in[sep_constraint]\n",
    "    alerts_matches = coo_alerts[idx_alerts[sep_constraint]]\n",
    "    alerts_matches_ids = alerts_id[idx_alerts[sep_constraint]]\n",
    "    \n",
    "    if trans_matches.size:\n",
    "        print(len(trans_matches), \"matches with separation -\", max_sep)\n",
    "        #sort_dist = np.sort(d2d_trans)\n",
    "        #print(\"Minimum distance found: \", sort_dist[0])\n",
    "\n",
    "    return trans_matches, alerts_matches, trans_matches_ids, alerts_matches_ids, sep_constraint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab DECAM ledger as pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving output to LCData_Legacy.csv\n"
     ]
    }
   ],
   "source": [
    "decam_transients = access_decam_data('https://datahub.geos.tamu.edu:8000/decam/LCData_Legacy/', overwrite = True) # If True, grabs a fresh batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving output to LCData_Legacy_AGN.csv\n"
     ]
    }
   ],
   "source": [
    "decam_transients_agn = access_decam_data('https://datahub.geos.tamu.edu:8000/decam/LCData_Legacy_AGN/', overwrite = True) # If True, grabs a fresh batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ObjectID</th>\n",
       "      <th>RA-OBJECT</th>\n",
       "      <th>DEC-OBJECT</th>\n",
       "      <th>NumberAlerts</th>\n",
       "      <th>MaxSCORE</th>\n",
       "      <th>RA-PSEUDO-HOST</th>\n",
       "      <th>DEC-PSEUDO-HOST</th>\n",
       "      <th>SEP-PSEUDO-HOST</th>\n",
       "      <th>RA-NEIGHBOR-STAR</th>\n",
       "      <th>DEC-NEIGHBOR-STAR</th>\n",
       "      <th>...</th>\n",
       "      <th>Discovery-Round</th>\n",
       "      <th>Discovery-Time</th>\n",
       "      <th>Discovery-Filter</th>\n",
       "      <th>Discovery-Magnitude</th>\n",
       "      <th>Discovery-SNR</th>\n",
       "      <th>Latest-Round</th>\n",
       "      <th>Latest-Time</th>\n",
       "      <th>Latest-Filter</th>\n",
       "      <th>Latest-Magnitude</th>\n",
       "      <th>Latest-SNR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A202103221407558m001825</td>\n",
       "      <td>211.982786</td>\n",
       "      <td>-0.306951</td>\n",
       "      <td>12</td>\n",
       "      <td>0.972</td>\n",
       "      <td>211.982614</td>\n",
       "      <td>-0.306946</td>\n",
       "      <td>0.6199</td>\n",
       "      <td>211.983372</td>\n",
       "      <td>-0.306315</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-22T06:40:19.074</td>\n",
       "      <td>N</td>\n",
       "      <td>22.13</td>\n",
       "      <td>19.2</td>\n",
       "      <td>9</td>\n",
       "      <td>2021-04-18T05:37:55.763</td>\n",
       "      <td>N</td>\n",
       "      <td>22.86</td>\n",
       "      <td>10.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A202103221408139m033502</td>\n",
       "      <td>212.057952</td>\n",
       "      <td>-3.583947</td>\n",
       "      <td>28</td>\n",
       "      <td>0.954</td>\n",
       "      <td>212.057864</td>\n",
       "      <td>-3.583960</td>\n",
       "      <td>0.3199</td>\n",
       "      <td>212.058798</td>\n",
       "      <td>-3.586276</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-22T08:20:58.209</td>\n",
       "      <td>N</td>\n",
       "      <td>21.78</td>\n",
       "      <td>25.7</td>\n",
       "      <td>23</td>\n",
       "      <td>2021-06-02T06:00:51.873</td>\n",
       "      <td>N</td>\n",
       "      <td>21.82</td>\n",
       "      <td>18.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A202103221408412p002445</td>\n",
       "      <td>212.171737</td>\n",
       "      <td>0.412527</td>\n",
       "      <td>49</td>\n",
       "      <td>0.998</td>\n",
       "      <td>212.171673</td>\n",
       "      <td>0.412394</td>\n",
       "      <td>0.5317</td>\n",
       "      <td>212.174697</td>\n",
       "      <td>0.411566</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-22T06:36:50.928</td>\n",
       "      <td>S</td>\n",
       "      <td>20.61</td>\n",
       "      <td>36.2</td>\n",
       "      <td>23</td>\n",
       "      <td>2021-06-02T02:59:21.867</td>\n",
       "      <td>S</td>\n",
       "      <td>21.46</td>\n",
       "      <td>33.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A202103221408578m005300</td>\n",
       "      <td>212.241200</td>\n",
       "      <td>-0.883300</td>\n",
       "      <td>2</td>\n",
       "      <td>0.855</td>\n",
       "      <td>212.241200</td>\n",
       "      <td>-0.883400</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>212.239800</td>\n",
       "      <td>-0.884900</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-22T08:17:30.880</td>\n",
       "      <td>S</td>\n",
       "      <td>22.55</td>\n",
       "      <td>15.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-03-24T07:10:51.368</td>\n",
       "      <td>S</td>\n",
       "      <td>22.33</td>\n",
       "      <td>25.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A202103221409059m023156</td>\n",
       "      <td>212.274757</td>\n",
       "      <td>-2.532478</td>\n",
       "      <td>21</td>\n",
       "      <td>0.969</td>\n",
       "      <td>212.274533</td>\n",
       "      <td>-2.532531</td>\n",
       "      <td>0.8290</td>\n",
       "      <td>212.275356</td>\n",
       "      <td>-2.535003</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-22T08:14:02.747</td>\n",
       "      <td>N</td>\n",
       "      <td>22.27</td>\n",
       "      <td>10.6</td>\n",
       "      <td>18</td>\n",
       "      <td>2021-05-18T03:17:17.544</td>\n",
       "      <td>N</td>\n",
       "      <td>22.99</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T202106021437535m004239</td>\n",
       "      <td>219.473273</td>\n",
       "      <td>-0.710944</td>\n",
       "      <td>6</td>\n",
       "      <td>0.973</td>\n",
       "      <td>219.473277</td>\n",
       "      <td>-0.711253</td>\n",
       "      <td>1.1100</td>\n",
       "      <td>219.473195</td>\n",
       "      <td>-0.714488</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>2021-06-02T02:37:33.855</td>\n",
       "      <td>N</td>\n",
       "      <td>21.46</td>\n",
       "      <td>30.6</td>\n",
       "      <td>24</td>\n",
       "      <td>2021-06-05T02:15:30.763</td>\n",
       "      <td>N</td>\n",
       "      <td>21.44</td>\n",
       "      <td>19.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T202106021440559m025615</td>\n",
       "      <td>220.233290</td>\n",
       "      <td>-2.937658</td>\n",
       "      <td>4</td>\n",
       "      <td>0.940</td>\n",
       "      <td>220.233839</td>\n",
       "      <td>-2.935933</td>\n",
       "      <td>6.5148</td>\n",
       "      <td>220.230799</td>\n",
       "      <td>-2.938535</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>2021-06-02T05:33:59.097</td>\n",
       "      <td>S</td>\n",
       "      <td>22.14</td>\n",
       "      <td>13.6</td>\n",
       "      <td>23</td>\n",
       "      <td>2021-06-02T05:40:21.261</td>\n",
       "      <td>S</td>\n",
       "      <td>22.36</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T202106021442286m035936</td>\n",
       "      <td>220.619578</td>\n",
       "      <td>-3.993391</td>\n",
       "      <td>3</td>\n",
       "      <td>0.935</td>\n",
       "      <td>220.618952</td>\n",
       "      <td>-3.993328</td>\n",
       "      <td>2.2614</td>\n",
       "      <td>220.615993</td>\n",
       "      <td>-3.991748</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>2021-06-02T05:33:59.097</td>\n",
       "      <td>N</td>\n",
       "      <td>22.15</td>\n",
       "      <td>22.5</td>\n",
       "      <td>23</td>\n",
       "      <td>2021-06-02T05:36:54.041</td>\n",
       "      <td>N</td>\n",
       "      <td>22.07</td>\n",
       "      <td>10.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T202106021449003p003225</td>\n",
       "      <td>222.251305</td>\n",
       "      <td>0.540528</td>\n",
       "      <td>7</td>\n",
       "      <td>0.984</td>\n",
       "      <td>222.251090</td>\n",
       "      <td>0.540740</td>\n",
       "      <td>1.0845</td>\n",
       "      <td>222.249251</td>\n",
       "      <td>0.544256</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>2021-06-02T02:27:20.976</td>\n",
       "      <td>S</td>\n",
       "      <td>21.04</td>\n",
       "      <td>47.1</td>\n",
       "      <td>24</td>\n",
       "      <td>2021-06-05T02:06:24.397</td>\n",
       "      <td>S</td>\n",
       "      <td>21.21</td>\n",
       "      <td>18.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T202106051435333m000534</td>\n",
       "      <td>218.888992</td>\n",
       "      <td>-0.092877</td>\n",
       "      <td>2</td>\n",
       "      <td>0.857</td>\n",
       "      <td>218.889396</td>\n",
       "      <td>-0.095045</td>\n",
       "      <td>7.9375</td>\n",
       "      <td>218.890974</td>\n",
       "      <td>-0.090694</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>2021-06-05T02:14:11.170</td>\n",
       "      <td>N</td>\n",
       "      <td>22.36</td>\n",
       "      <td>21.7</td>\n",
       "      <td>24</td>\n",
       "      <td>2021-06-05T02:15:30.763</td>\n",
       "      <td>N</td>\n",
       "      <td>21.86</td>\n",
       "      <td>14.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>557 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ObjectID   RA-OBJECT  DEC-OBJECT  NumberAlerts  MaxSCORE  \\\n",
       "0   A202103221407558m001825  211.982786   -0.306951            12     0.972   \n",
       "0   A202103221408139m033502  212.057952   -3.583947            28     0.954   \n",
       "0   A202103221408412p002445  212.171737    0.412527            49     0.998   \n",
       "0   A202103221408578m005300  212.241200   -0.883300             2     0.855   \n",
       "0   A202103221409059m023156  212.274757   -2.532478            21     0.969   \n",
       "..                      ...         ...         ...           ...       ...   \n",
       "0   T202106021437535m004239  219.473273   -0.710944             6     0.973   \n",
       "0   T202106021440559m025615  220.233290   -2.937658             4     0.940   \n",
       "0   T202106021442286m035936  220.619578   -3.993391             3     0.935   \n",
       "0   T202106021449003p003225  222.251305    0.540528             7     0.984   \n",
       "0   T202106051435333m000534  218.888992   -0.092877             2     0.857   \n",
       "\n",
       "    RA-PSEUDO-HOST  DEC-PSEUDO-HOST  SEP-PSEUDO-HOST  RA-NEIGHBOR-STAR  \\\n",
       "0       211.982614        -0.306946           0.6199        211.983372   \n",
       "0       212.057864        -3.583960           0.3199        212.058798   \n",
       "0       212.171673         0.412394           0.5317        212.174697   \n",
       "0       212.241200        -0.883400           0.3000        212.239800   \n",
       "0       212.274533        -2.532531           0.8290        212.275356   \n",
       "..             ...              ...              ...               ...   \n",
       "0       219.473277        -0.711253           1.1100        219.473195   \n",
       "0       220.233839        -2.935933           6.5148        220.230799   \n",
       "0       220.618952        -3.993328           2.2614        220.615993   \n",
       "0       222.251090         0.540740           1.0845        222.249251   \n",
       "0       218.889396        -0.095045           7.9375        218.890974   \n",
       "\n",
       "    DEC-NEIGHBOR-STAR  ...  Discovery-Round           Discovery-Time  \\\n",
       "0           -0.306315  ...                0  2021-03-22T06:40:19.074   \n",
       "0           -3.586276  ...                0  2021-03-22T08:20:58.209   \n",
       "0            0.411566  ...                0  2021-03-22T06:36:50.928   \n",
       "0           -0.884900  ...                0  2021-03-22T08:17:30.880   \n",
       "0           -2.535003  ...                0  2021-03-22T08:14:02.747   \n",
       "..                ...  ...              ...                      ...   \n",
       "0           -0.714488  ...               23  2021-06-02T02:37:33.855   \n",
       "0           -2.938535  ...               23  2021-06-02T05:33:59.097   \n",
       "0           -3.991748  ...               23  2021-06-02T05:33:59.097   \n",
       "0            0.544256  ...               23  2021-06-02T02:27:20.976   \n",
       "0           -0.090694  ...               24  2021-06-05T02:14:11.170   \n",
       "\n",
       "   Discovery-Filter Discovery-Magnitude  Discovery-SNR  Latest-Round  \\\n",
       "0                 N               22.13           19.2             9   \n",
       "0                 N               21.78           25.7            23   \n",
       "0                 S               20.61           36.2            23   \n",
       "0                 S               22.55           15.5             1   \n",
       "0                 N               22.27           10.6            18   \n",
       "..              ...                 ...            ...           ...   \n",
       "0                 N               21.46           30.6            24   \n",
       "0                 S               22.14           13.6            23   \n",
       "0                 N               22.15           22.5            23   \n",
       "0                 S               21.04           47.1            24   \n",
       "0                 N               22.36           21.7            24   \n",
       "\n",
       "                Latest-Time Latest-Filter Latest-Magnitude  Latest-SNR  \n",
       "0   2021-04-18T05:37:55.763             N            22.86        10.2  \n",
       "0   2021-06-02T06:00:51.873             N            21.82        18.1  \n",
       "0   2021-06-02T02:59:21.867             S            21.46        33.9  \n",
       "0   2021-03-24T07:10:51.368             S            22.33        25.6  \n",
       "0   2021-05-18T03:17:17.544             N            22.99         8.8  \n",
       "..                      ...           ...              ...         ...  \n",
       "0   2021-06-05T02:15:30.763             N            21.44        19.8  \n",
       "0   2021-06-02T05:40:21.261             S            22.36        14.0  \n",
       "0   2021-06-02T05:36:54.041             N            22.07        10.5  \n",
       "0   2021-06-05T02:06:24.397             S            21.21        18.4  \n",
       "0   2021-06-05T02:15:30.763             N            21.86        14.9  \n",
       "\n",
       "[557 rows x 21 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decam_transients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run initial check (on tiles) and closer check (on targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 matches with separation - 1.8 deg\n",
      "1 matches with separation - 1.8 deg\n",
      "8 matches with separation - 1.8 deg\n",
      "5 matches with separation - 1.8 deg\n",
      "10 matches with separation - 1.8 deg\n",
      "2 matches with separation - 1.8 deg\n",
      "6 matches with separation - 1.8 deg\n",
      "9 matches with separation - 1.8 deg\n",
      "2 matches with separation - 1.8 deg\n",
      "6 matches with separation - 1.8 deg\n",
      "6 matches with separation - 1.8 deg\n",
      "3 matches with separation - 1.8 deg\n",
      "4 matches with separation - 1.8 deg\n",
      "12 matches with separation - 1.8 deg\n",
      "9 matches with separation - 1.8 deg\n",
      "5 matches with separation - 1.8 deg\n",
      "3 matches with separation - 1.8 deg\n",
      "3 matches with separation - 1.8 deg\n",
      "1 matches with separation - 1.8 deg\n",
      "1 matches with separation - 1.8 deg\n",
      "7 matches with separation - 1.8 deg\n",
      "3 matches with separation - 1.8 deg\n",
      "8 matches with separation - 1.8 deg\n",
      "8 matches with separation - 1.8 deg\n",
      "9 matches with separation - 1.8 deg\n",
      "12 matches with separation - 1.8 deg\n",
      "10 matches with separation - 1.8 deg\n",
      "4 matches with separation - 1.8 deg\n",
      "8 matches with separation - 1.8 deg\n",
      "1 matches with separation - 1.8 deg\n",
      "4 matches with separation - 1.8 deg\n",
      "4 matches with separation - 1.8 deg\n",
      "6 matches with separation - 1.8 deg\n",
      "6 matches with separation - 1.8 deg\n",
      "3 matches with separation - 1.8 deg\n",
      "3 matches with separation - 1.8 deg\n",
      "4 matches with separation - 1.8 deg\n",
      "1 matches with separation - 1.8 deg\n",
      "5 matches with separation - 1.8 deg\n",
      "4 matches with separation - 1.8 deg\n",
      "5 matches with separation - 1.8 deg\n",
      "1 matches with separation - 1.8 deg\n",
      "2 matches with separation - 1.8 deg\n",
      "4 matches with separation - 1.8 deg\n",
      "4 matches with separation - 1.8 deg\n",
      "3 matches with separation - 1.8 deg\n",
      "3 matches with separation - 1.8 deg\n",
      "3 matches with separation - 1.8 deg\n",
      "2 matches with separation - 1.8 deg\n",
      "1 matches with separation - 1.8 deg\n",
      "1 matches with separation - 1.8 deg\n",
      "1 matches with separation - 1.8 deg\n"
     ]
    }
   ],
   "source": [
    "init_matches_by_date = initial_check(ledger_df = decam_transients, ledger_type = 'DECAM_TAMU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 20210322\n",
      "8 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210402\n",
      "2 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210405\n",
      "NaNs found, removing them from array before match.\n",
      "24 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210406\n",
      "NaNs found, removing them from array before match.\n",
      "15 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210407\n",
      "NaNs found, removing them from array before match.\n",
      "33 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210408\n",
      "NaNs found, removing them from array before match.\n",
      "4 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210409\n",
      "NaNs found, removing them from array before match.\n",
      "7 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210410\n",
      "NaNs found, removing them from array before match.\n",
      "40 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210411\n",
      "NaNs found, removing them from array before match.\n",
      "\n",
      " 20210412\n",
      "NaNs found, removing them from array before match.\n",
      "5 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210413\n",
      "NaNs found, removing them from array before match.\n",
      "9 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210414\n",
      "NaNs found, removing them from array before match.\n",
      "9 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210415\n",
      "NaNs found, removing them from array before match.\n",
      "3 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210416\n",
      "NaNs found, removing them from array before match.\n",
      "20 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210417\n",
      "NaNs found, removing them from array before match.\n",
      "12 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210418\n",
      "NaNs found, removing them from array before match.\n",
      "6 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210419\n",
      "NaNs found, removing them from array before match.\n",
      "6 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210420\n",
      "NaNs found, removing them from array before match.\n",
      "4 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210428\n",
      "NaNs found, removing them from array before match.\n",
      "\n",
      " 20210429\n",
      "NaNs found, removing them from array before match.\n",
      "1 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210430\n",
      "NaNs found, removing them from array before match.\n",
      "33 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210501\n",
      "NaNs found, removing them from array before match.\n",
      "4 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210502\n",
      "NaNs found, removing them from array before match.\n",
      "46 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210503\n",
      "NaNs found, removing them from array before match.\n",
      "33 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210504\n",
      "NaNs found, removing them from array before match.\n",
      "35 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210505\n",
      "NaNs found, removing them from array before match.\n",
      "40 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210506\n",
      "32 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210507\n",
      "16 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210508\n",
      "30 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210509\n",
      "2 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210510\n",
      "22 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210511\n",
      "1 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210512\n",
      "9 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210513\n",
      "12 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210517\n",
      "8 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210518\n",
      "3 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210529\n",
      "NaNs found, removing them from array before match.\n",
      "13 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210530\n",
      "\n",
      " 20210531\n",
      "1 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210602\n",
      "5 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210604\n",
      "1 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210605\n",
      "\n",
      " 20210606\n",
      "\n",
      " 20210608\n",
      "3 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210609\n",
      "10 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210610\n",
      "2 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210611\n",
      "\n",
      " 20210612\n",
      "9 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210613\n",
      "5 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210627\n",
      "2 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210706\n",
      "\n",
      " 20210709\n"
     ]
    }
   ],
   "source": [
    "close_matches = closer_check(init_matches_by_date, ledger_df = decam_transients, ledger_type = 'DECAM_TAMU', exclusion_list = [])\n",
    "np.save('matches_DECam',close_matches, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 matches with separation - 1.8 deg\n",
      "1 matches with separation - 1.8 deg\n",
      "8 matches with separation - 1.8 deg\n",
      "5 matches with separation - 1.8 deg\n",
      "10 matches with separation - 1.8 deg\n",
      "2 matches with separation - 1.8 deg\n",
      "6 matches with separation - 1.8 deg\n",
      "9 matches with separation - 1.8 deg\n",
      "2 matches with separation - 1.8 deg\n",
      "6 matches with separation - 1.8 deg\n",
      "6 matches with separation - 1.8 deg\n",
      "3 matches with separation - 1.8 deg\n",
      "4 matches with separation - 1.8 deg\n",
      "12 matches with separation - 1.8 deg\n",
      "9 matches with separation - 1.8 deg\n",
      "5 matches with separation - 1.8 deg\n",
      "3 matches with separation - 1.8 deg\n",
      "3 matches with separation - 1.8 deg\n",
      "1 matches with separation - 1.8 deg\n",
      "1 matches with separation - 1.8 deg\n",
      "7 matches with separation - 1.8 deg\n",
      "3 matches with separation - 1.8 deg\n",
      "8 matches with separation - 1.8 deg\n",
      "8 matches with separation - 1.8 deg\n",
      "9 matches with separation - 1.8 deg\n",
      "12 matches with separation - 1.8 deg\n",
      "10 matches with separation - 1.8 deg\n",
      "4 matches with separation - 1.8 deg\n",
      "8 matches with separation - 1.8 deg\n",
      "1 matches with separation - 1.8 deg\n",
      "4 matches with separation - 1.8 deg\n",
      "4 matches with separation - 1.8 deg\n",
      "6 matches with separation - 1.8 deg\n",
      "6 matches with separation - 1.8 deg\n",
      "3 matches with separation - 1.8 deg\n",
      "3 matches with separation - 1.8 deg\n",
      "4 matches with separation - 1.8 deg\n",
      "1 matches with separation - 1.8 deg\n",
      "4 matches with separation - 1.8 deg\n",
      "4 matches with separation - 1.8 deg\n",
      "5 matches with separation - 1.8 deg\n",
      "1 matches with separation - 1.8 deg\n",
      "2 matches with separation - 1.8 deg\n",
      "3 matches with separation - 1.8 deg\n",
      "4 matches with separation - 1.8 deg\n",
      "3 matches with separation - 1.8 deg\n",
      "3 matches with separation - 1.8 deg\n",
      "3 matches with separation - 1.8 deg\n",
      "2 matches with separation - 1.8 deg\n",
      "1 matches with separation - 1.8 deg\n",
      "1 matches with separation - 1.8 deg\n",
      "1 matches with separation - 1.8 deg\n",
      "\n",
      " 20210322\n",
      "\n",
      " 20210402\n",
      "\n",
      " 20210405\n",
      "NaNs found, removing them from array before match.\n",
      "24 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210406\n",
      "NaNs found, removing them from array before match.\n",
      "27 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210407\n",
      "NaNs found, removing them from array before match.\n",
      "27 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210408\n",
      "NaNs found, removing them from array before match.\n",
      "8 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210409\n",
      "NaNs found, removing them from array before match.\n",
      "10 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210410\n",
      "NaNs found, removing them from array before match.\n",
      "18 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210411\n",
      "NaNs found, removing them from array before match.\n",
      "\n",
      " 20210412\n",
      "NaNs found, removing them from array before match.\n",
      "6 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210413\n",
      "NaNs found, removing them from array before match.\n",
      "7 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210414\n",
      "NaNs found, removing them from array before match.\n",
      "4 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210415\n",
      "NaNs found, removing them from array before match.\n",
      "3 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210416\n",
      "NaNs found, removing them from array before match.\n",
      "8 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210417\n",
      "NaNs found, removing them from array before match.\n",
      "18 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210418\n",
      "NaNs found, removing them from array before match.\n",
      "5 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210419\n",
      "NaNs found, removing them from array before match.\n",
      "2 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210420\n",
      "NaNs found, removing them from array before match.\n",
      "3 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210428\n",
      "NaNs found, removing them from array before match.\n",
      "\n",
      " 20210429\n",
      "NaNs found, removing them from array before match.\n",
      "\n",
      " 20210430\n",
      "NaNs found, removing them from array before match.\n",
      "7 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210501\n",
      "NaNs found, removing them from array before match.\n",
      "2 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210502\n",
      "NaNs found, removing them from array before match.\n",
      "15 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210503\n",
      "NaNs found, removing them from array before match.\n",
      "10 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210504\n",
      "NaNs found, removing them from array before match.\n",
      "13 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210505\n",
      "NaNs found, removing them from array before match.\n",
      "9 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210506\n",
      "8 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210507\n",
      "5 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210508\n",
      "7 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210509\n",
      "1 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210510\n",
      "3 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210511\n",
      "8 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210512\n",
      "12 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210513\n",
      "1 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210517\n",
      "3 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210518\n",
      "6 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210529\n",
      "NaNs found, removing them from array before match.\n",
      "2 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210530\n",
      "6 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210531\n",
      "5 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210602\n",
      "9 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210604\n",
      "8 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210605\n",
      "\n",
      " 20210606\n",
      "\n",
      " 20210608\n",
      "3 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210609\n",
      "5 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210610\n",
      "2 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210611\n",
      "2 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210612\n",
      "2 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210613\n",
      "1 matches with separation - 2.0 arcsec\n",
      "\n",
      " 20210627\n",
      "\n",
      " 20210706\n",
      "\n",
      " 20210709\n"
     ]
    }
   ],
   "source": [
    "init_matches_agn_by_date = initial_check(ledger_df = decam_transients_agn, ledger_type = 'DECAM_TAMU')\n",
    "close_matches_agn = closer_check(init_matches_agn_by_date, ledger_df = decam_transients_agn, ledger_type = 'DECAM_TAMU', exclusion_list = [])\n",
    "np.save('matches_DECam_agn',close_matches_agn, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('matches_DECam_agn',close_matches_agn, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick plot to see the distribution of target matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2aaae5391250>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(close_matches[:,4], close_matches[:,5],label='SN')\n",
    "plt.scatter(close_matches_agn[:,4], close_matches_agn[:,5],label='AGN')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End notes:\n",
    "Double matches are to be expected, could be worthwhile to compare the spectra of both"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DESI master",
   "language": "python",
   "name": "desi-master"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
