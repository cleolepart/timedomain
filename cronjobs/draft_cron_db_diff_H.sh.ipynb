{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "## This is similar to cron_test.sh but is not called with sbatch, \n",
    "## Because it runs sbatch itself\n",
    "## It interacts with the sqlite3 db called \n",
    "## /global/cfs/cdirs/desi/science/td/daily-search/transients_search.db\n",
    "## Blame Antonella Palmese version Jan 2021 for the ugliness of this code\n",
    "## First draft reworking to fix db reading issues with DESIDIFF\n",
    "## Many ??\n",
    "## 06/04/21 11:56:00 Cleo Lepart\n",
    "\n",
    "### set -e\n",
    "echo `date` Running daily time domain pipeline on `hostname`\n",
    "#- Configure desi environment if needed\n",
    "if [ -z \"$DESI_ROOT\" ]; then\n",
    "    echo \"Loading DESI modules\"\n",
    "    module use /global/common/software/desi/$NERSC_HOST/desiconda/startup/modulefiles\n",
    "    echo \"module load\"\n",
    "    module load desimodules/master\n",
    "fi\n",
    "\n",
    "tiles_path=\"/global/project/projectdirs/desi/spectro/redux/daily/tiles\" 
    "\n",
    "run_path=\"/global/cscratch1/sd/akim/project/timedomain/cronjobs/\" 
    "\n",
    "td_path=\"/global/cfs/cdirs/desi/science/td/daily-search/\" 
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ##################
#Now double check that we have run over all new exposures. If not, we send new runs.
#The database is updated in the next lines starting where it calls python ${run_path}exposure_db.py
#That script updates the exposures table, which is later compared to the processed exposures in 
#desitrip_exposures to find unprocessed exposures. 
#desitrip_exposures is then updated with new exposures that went through the classifier in the classifier script
#ATM this only works for desitrip outputs - needs to be added to specdiff

echo "Looking for new exposures"


python ${run_path}exposure_db.py daily #creates exposures db cf. below from transients_search.db cf. /cronjobs/exposure_db.py L162 ?

query="select distinct obsdate,tileid from exposures where (tileid,obsdate) not in (select tileid,obsdate from desidiff_H_coadd_exposures);" 
# query="select distinct obsdate,tileid from exposures
# where (tileid,obsdate) not in (select tileid,obsdate from desidiff_cv_coadd_exposures);" 
mapfile -t -d $'\n' obsdates_tileids < <( sqlite3 ${td_path}transients_search.db "$query" ) #queries transients_search.db 


query_tileid_only = "select distinct tileid from exposures where (tileid,obsdate) not in (select tileid,obsdate from desidiff_H_coadd_exposures);" #AND obsdate>20210529;"
#queries transcients_search for unprocessed (new) tileids only

mapfile -t -d $'\n' tileids < < ( sqlite3 ${td_path}transients_search.db "$query_tileid_only" )
#array of unprocessed tileids

desi_path="/global/cfs/cdirs/desi/science/td/db/"
#path needed for association of tileid (@transcients_search.db) to targetid (@desi.db)
#why not only use desi.db for this process? 

desi_query="select distinct TARGETID, YYYYMMDD from fibermap_daily;"
#command within for loop for fetching and insertion of targetid with associated tileid

for id in ${tileid[@]}
    mapfile -t targetid_arr < <( sqlite3 ${desi_path}desi.db "$query_desi")
    #try insert with loop

   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare sbatch file\n",
    "now=$( date -d \"today\" '+%Y%m%d_%T' )\n",
    "logfile=\"${td_path}desitrip/log/${now}.log\"\n",
    "echo \"Putting log into \"$logfile\n",
    "\n",
    "#I think this is not clever at the moment: it sends different jobs for each obsdate,\n",
    "#but most of the time is spent importing tensorflow. \n",
    "#It would be more clever to run multiple obsdate together\n",
    "\n",
    "echo \"#!/bin/bash\n",
    "#SBATCH --qos=regular\n",
    "#SBATCH --time=120\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --tasks-per-node=1\n",
    "#SBATCH --output=$logfile\n",
    "#SBATCH --constraint=haswell\n",
    "\">${run_path}sbatch_file.sh\n",
    "\n",
    "Nobsdates_tileids=${#obsdates_tileids[@]}\n",
    "if [ $Nobsdates_tileids -eq 0 ]; then\n",
    "    echo \"No new observations found today\"\n",
    "else\n",
    "    echo \"$Nobsdates_tileids new observations found\"\n",
    "\n",
    "    echo \"---------- Starting coadd differencing ----------\"\n",
    "    \n",
    "#don't think that anything needs to be changed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo \"---------- Starting coadd differencing ----------\"\n",
    "\n",
    "\n",
    "    \n",
    "    run_path_diff=\"/global/cscratch1/sd/akim/project/timedomain/timedomain/bin/\"\n",
    "    logfile=\"${td_path}/desitrip/log/${now}.log\"\n",
    "\n",
    "    # instead of doing all the date/tile pairs at once, split into pieces\n",
    "    # the purpose is to have more things get processed/saved in case of\n",
    "    # any kind of error\n",
    "    nper=1\n",
    "    nloop=$(((Nobsdates_tileids+nper-1)/nper))\n",
    "\n",
    "    for ((i=0;i<$nloop;i++)); \n",
    "        do \n",
    "            subarr=(\"${obsdates_tileids[@]:$(($i*$nper)):$nper}\")\n",
    "            echo \"${subarr[@]}\"\n",
    "            \n",
    "#running from binary? \n",
    "#refers to desitrip, not desidiff\n",
    "#don't think that anything needs to be changed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     echo \"${run_path_diff}/diff-db.py $lastnite CVLogic Date_SpectraPairs_Iterator daily\n",
    "    #     coadd\"\n",
    "    #     srun -o ${logfile} ${run_path_diff}/diff.py $lastnite CVLogic\n",
    "    #     Date_SpectraPairs_Iterator daily coadd\n",
    "    \n",
    "            python ${run_path_diff}diff-db.py TileDate_SpectraPairs_Iterator HydrogenLogic recent coadd --obsdates_tilenumbers ${subarr[@]}\n",
    "#             python ${run_path_diff}diff-db.py TileDate_SpectraPairs_Iterator CVLogic daily coadd --obsdates_tilenumbers ${subarr[@]}\n",
    "            if [ $? -eq 0 ]\n",
    "            then\n",
    "                echo \"Successfully executed script\"\n",
    "                #Now add this tile info to the sqlite db #into the processed db, right?\n",
    "                for t in ${subarr[@]}; do\n",
    "                    arrt=(${t//|/ })\n",
    "                    query=\"INSERT OR IGNORE INTO desidiff_H_coadd_exposures(obsdate, tileid) VALUES(${arrt[0]},${arrt[1]});\"\n",
    "#                     query=\"INSERT OR IGNORE INTO desidiff_cv_coadd_exposures(obsdate, tileid) VALUES(${arrt[0]},${arrt[1]});\"\n",
    "                    echo $query\n",
    "                    sqlite3 ${td_path}transients_search.db \"$query\" ##assigning processed targets to transients_search.db which needs new table\n",
    "                done\n",
    "            else\n",
    "              # Redirect stdout from echo command to stderr.\n",
    "              echo \"Script encountered error.\" >&2\n",
    "#               echo \"Failure in $query\" |  mail -s 'Failure: cron_db_diff.sh' agkim@lbl.gov\n",
    "              exit 1\n",
    "            fi\n",
    "        done\n",
    "\n",
    "fi\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
